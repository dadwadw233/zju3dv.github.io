<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>OnePose: One-Shot Object Pose Estimation without CAD Models</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Long-term Visual Localization with Mobile Sensors </h2>
            <h4 style="color:#6e6e6e;">CVPR 2022</h4>
            <!-- <h5 style="color:#6e6e6e;"> (Oral Presentation and Best Paper Candidate)</h5> -->
            <hr>
            <h6> <a target="_blank">Shen Yan</a><sup>1,3</sup>, 
                 <a target="_blank">Yu Liu</a><sup>1</sup>, 
                 <a href="https://github.com/WangLongZJU" target="_blank">Long Wang</a><sup>2</sup>, 
                <a target="_blank">Zhen Peng</a><sup>2</sup>,
                <a target="_blank">Haomin Liu</a><sup>2</sup>,
                <a target="_blank">Maojun Zhang</a><sup>1</sup>,
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>3</sup>,
                <a href="https://xzhou.me" target="_blank">Xiaowei Zhou</a><sup>3</sup>
            </h6>
            <p> <sup>1</sup>National University of Defense Technology &nbsp;&nbsp; 
                <sup>2</sup>SenseTime Research &nbsp;&nbsp;
                <sup>3</sup>Zhejiang University &nbsp;&nbsp;
                <br>
                <!-- <sup>*</sup> denotes equal contribution -->
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="dataset" href="" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Dataset </a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <h6 style="color:#8899a5"> TL;DR: </h6>
<!--             <video poster="images/header-vid-poster.png" width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid"> -->
            <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
                  <source src="videos/DeepAC_demo.mp4" type="video/mp4">
            </video> -->
            <!-- <video id="demo" width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls="" id="inspect_vid">
                  <source src="videos/DeepAC_demo.mp4" type="video/mp4">
            </video> -->

            <div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
            <br>
            </div>
              <!-- <br><br> -->
          <p class="text-justify">
            Despite the remarkable advances in image matching and pose estimation, image-based localization of a camera in a temporally-varying outdoor environment is still a challenging problem due to huge appearance disparity between query and reference images caused by illumination, seasonal and structural changes. In this work, we propose to leverage additional sensors on a mobile phone, mainly GPS, compass, and gravity sensor, to solve this challenging problem. We show that these mobile sensors provide decent initial poses and effective constraints to reduce the searching space in image matching and final pose estimation. With the initial pose, we are also able to devise a direct 2D-3D matching network to efficiently establish 2D-3D correspondences instead of tedious 2D-2D matching in existing systems. As no public dataset exists for the studied problem, we collect a new dataset that provides a variety of mobile sensor data and significant scene appearance variations, and develop a system to acquire ground-truth poses for query images. We benchmark our method as well as several state-of-the-art baselines and demonstrate the effectiveness of the proposed approach. The code and dataset will be released publicly.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- overview video -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Overview video (5 min)</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/wuMPaUTJuO0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
            </div>
        </div>
      </div>
    </div>
  </section>
  <br> -->


  <!-- Pipeline overview -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Pipeline overview</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/onepose-arch.png" alt="NeuralRecon Architechture">
            <hr style="margin-top:0px">
            <p class="text-justify">
              $\textbf{1.}$ For each object, a video scan with RGB frames $\{\mathbf{I}_i\}$ and camera poses $\{\xi_{i}\}$ are collected together with the annotated 3D object bounding box $\mathbf{B}$.
              $\textbf{2.}$ Structure from Motion (SfM) reconstructs a sparse point cloud $\{\mathbf{P}_j\}$ of the object.
              $\textbf{3.}$ The correspondence graphs $\{\mathcal{G}_j\}$  are built during SfM, which represent the 2D-3D correspondences in the SfM map.
              $\textbf{4.}$ 2D descriptors $\{\mathbf{F}_k^{2D}\}$ are aggregated to 3D descriptors $\{\mathbf{F}_j^{3D}\}$  with the aggregration-attention layer. $\{\mathbf{F}_j^{3D}\}$ are later matched with 2D descriptors from the query image $\{\mathbf{F}_q^{2D}\}$ to generate 2D-3D match predictions $\mathcal{M}_{3D}$.
              $\textbf{5.}$ Finally, the object pose $\xi_{q}$  is computed by solving the PnP problem with $\mathcal{M}_{3D}$.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br> -->

    <!-- Comparison with Atlas -->
    <!-- <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
              <h3>Comparison with <a href="http://zak.murez.com/atlas">Atlas</a> on a large scene (30m x 10m)</h3>
              <hr style="margin-top:0px">
              <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls="">
                <source src="videos/Comparison-atlas.m4v" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </section>
    <br>   -->

  <!-- Comparison with depth-based methods -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with depth-based methods</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/compare-depth-based.png" alt="Comparison with depth-based methods">
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgement</h3>
          <hr style="margin-top:0px">
          <p>
            We would like to specially thank to Reviewer 3 for the positive and constructive comments.
          </p>
          <hr>
      </div>
    </div>
  </div> -->

  <!-- citing -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
  @article{sun2022onepose,
    title={{OnePose}: One-Shot Object Pose Estimation without {CAD} Models},
    author = {Sun, Jiaming and Wang, Zihao and Zhang, Siyu and He, Xingyi and Zhao, Hongcheng and Zhang, Guofeng and Zhou, Xiaowei},
    journal={CVPR},
    year={2022},
   }
</code></pre>
      </div>
    </div>
  </div> -->

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            We would like to specially thank Reviewer 3 for the insightful and constructive comments.
            We would like to thank Sida Peng , Siyu Zhang and Qi Fang for the proof-reading.
          </p>
      </div>
    </div>
  </div> -->

  <!-- rec -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works from our group</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout our work on Transformer-based feature matching (<a href="http://zju3dv.github.io/loftr">LoFTR</a>) and human reconstruction (<a href="http://zju3dv.github.io/neuralbody">NeuralBody</a> and <a href="http://zju3dv.github.io/Mirrored-Human">Mirrored-Human</a>) in CVPR 2021.
          </p>
      </div>
    </div>
  </div> -->

  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>

