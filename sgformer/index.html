<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="NeuMesh encodes the neural implicit field with disentangled geometry and texture features on a mesh scaffold, thereby enables mesh-guided geometry deformation, texture swapping, filling and painting.">
  <meta name="keywords" content="NeuMesh, Neural Rendering, Scene Editing, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="./static/images/thumbnail.png"/>
  <link rel="icon"
        type="image/x-icon"
        href="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/satellite_1.png"/>

  <title>SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion
    </title>


  </script>

  <!-- <script type="module"
          src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <!-- <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <!-- <a class="navbar-item" href="https://zju3dv.github.io/cp-slam">
            CP-SLAM
          </a> -->
          <!-- <a class="navbar-item" href="https://zju3dv.github.io/nr_in_a_room">
            Neural Rendering in a Room
          </a>
          <a class="navbar-item" href="https://zju3dv.github.io/neural_outdoor_rerender">
            Neural Outdoor Re-Rendering
          </a> -->
          <!-- <a class="navbar-item" href="https://zju3dv.github.io/latent_human">
            LatentHuman
          </a> -->
          <!-- <a class="navbar-item" href="https://zju3dv.github.io/sine">
            SINE
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-5 has-text-centered">
          <img style="width: 1000%; transform: translate(1%, 0%);" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/title.jpg" alt="SGFormer"/>
        </div>
      </div>
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">
        Satellite-Ground Fusion for 3D Semantic Scene Completion
        </h1>
        <h1 class="title is-size-3" style="color:#5a6268;">CVPR 2025</h1>
        <div class="is-size-5 publication-authors">
          <div class="author-block">
            [<a href="https://github.com/gxytcrc">Xiyue Guo</a><sup>1</sup>,</div>
          <div class="author-block">
            <a href="https://github.com/hjr37">Jiarui Hu</a><sup>1</sup>]<sup>Co-Authors</sup>,</div>
          <div class="author-block">
            <a href="https://junjh.github.io/">Junjie Hu</a><sup>2</sup>,</div>
          <div class="author-block">
            <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a><sup>1</sup>,
          </div>
          <div class="author-block">
            <a href="http://www.cad.zju.edu.cn/home/gfzhang/">Guofeng Zhang*</a><sup>1</sup>
          </div>
        </div>

        <div class="is-size-5 publication-authors">
          <!-- * denotes equal contribution <br> -->
          <span class="author-block"><sup>1</sup>State Key Lab of CAD & CG, Zhejiang University,</span>
          <span class="author-block"><sup>2</sup>Chinese University of Hong Kong, Shenzhen</span>
        </div>

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
              <a href="https://arxiv.org/abs/2503.16825"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
          <span class="link-block">
              <a href="https://arxiv.org/abs/2503.16825"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
          </span>
          <span class="link-block">
            <a href="https://github.com/gxytcrc/SGFormer"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
              </a>
          </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- <video id="teaser" autoplay controls muted loop playsinline height="100%">
        <source src="https://raw.githubusercontent.com/ybbbbt/open_access_assets/main/neumesh/hybrid_edit.mp4"
                type="video/mp4">
      </video> -->
      <img style="width: 100%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/teaser.jpg"
             alt="NeuMesh architecture."/>
      <!-- <h2 class="subtitle has-text-centered"> -->
      <div class="content has-text-justified">
        <p>
          <i>SGFormer</i>, which adopts satellite-ground cooperative fusion, can achieve state-of-the-art performance in scene completion and semantic prediction. Benefiting from informative satellite images and a well-designed dual-branch pipeline, SGFormer can improve semantic prediction accuracy and solve the long-standing visual occlusion bottleneck suffered by purely ground-view methods.
        </p>
      </div>
      <!-- </h2> -->
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fifth-sixths">
        <h2 class="title is-3" style="color:#ffa200;">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recently, camera-based solutions have been extensively explored for scene semantic completion (SSC). Despite their success in visible areas, existing methods struggle to capture complete scene semantics due to frequent visual occlusions. 
            To address this limitation, this paper presents the first satellite-ground cooperative SSC framework, i.e., SGFormer, exploring the potential of satellite-ground image pairs in the SSC task. 
            Specifically, we propose a dual-branch architecture that encodes orthogonal satellite and ground views in parallel, unifying them into a common domain. Additionally, we design a ground-view guidance strategy that corrects satellite image biases during feature encoding, addressing misalignment between satellite and ground views. Moreover, we develop an adaptive weighting strategy that balances contributions from satellite and ground views. Experiments demonstrate that SGFormer outperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360 datasets.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="color:#ffa200;">Video</h2>
        <!-- <h2 class="title is-5">YouTube Source</h2> -->
        <div class="publication-video">
          <!-- <iframe width="640" height="480" src="./video/cg-slam-show.mp4"
                  title="YouTube video player" frameborder="0"
                  allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen></iframe> -->
          <video id="functions" autoplay controls muted loop playsinline height="100%">
            <source src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/video/sgformer.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<hr/>

<section class="section">

  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <h2 class="title is-3 has-text-centered" style="color:#ffa200;"><p>Framework Overview</p></h2>
      <div class="has-text-centered">
        <img style="width: 100%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/pipeline.jpg"
             alt="CG-SLAM Overview."/>
      </div>
      <p>
        Overall, SGFormer feeds a satellite-ground image pair into similar backbone networks in different branches to extract multi-level feature maps respectively <strong>(Left Part)</strong>. 
        Then, leveraging deformable attention, it transforms satellite and ground features into volume and BEV spaces <strong>(Middle Part)</strong> for following feature fusion and decoding <strong>(Right Part)</strong>. 
        Specifically, in the ground branch, we use a depth estimator to produce voxel proposals for targeted querying on non-empty feature volumes. 
        In the satellite branch, we fuse vertically squeezed ground-view features into BEV queries to warm up satellite features <strong>(Red Line)</strong>.
        Before final fusion, encoded features from both branches are enhanced through 2D/3D convolution networks. Our proposed fusion module, which is detailed in the above figure, is able to adaptively fuse satellite and ground features, followed by a seg head to output semantic reconstruction results.
      </p>
    </div>
  </div>

  <br/>

    <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <h2 class="title is-3 has-text-centered" style="color:#ffa200;"><p>Scene Semantic Completion &rarr; Qualitative Performance</p></h2>
      <div class="has-text-centered">
        <div class="has-text-centered">
          <img style="width: 100%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/fig.jpg"
          alt="ssc."/>
        </div>
      </div>
      <p>
        <strong><font size="4">  Annotations:</font> </strong> <strong>(Left)</strong>: we show satellite-ground image pairs and indicate the vehicle's travel direction as a yellow arrow. 
        <strong>(Right)</strong>: we qualitatively compare scene semantic completion results from SGFormer and other baselines, where SGFormer can produce more complete and accurate semantic reconstruction relying on the satellite-ground fusion.
      </p>
      <div class="has-text-centered">
        <video style="width: 70%;" id="functions" autoplay controls muted loop playsinline height="100%">
          <source src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/video/semantickitti_cut.mp4"
                  type="video/mp4">
        </video>
        <p>
          <strong><font size="4">  Annotations:</font></strong> Qualitative results on the validation set of SemanticKITTI.
        </p>
        <video style="width: 70%;" id="functions" autoplay controls muted loop playsinline height="100%">
          <source src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/video/Kitti_360.mp4"
                  type="video/mp4">
        </video>
        <p>
          <strong><font size="4">  Annotations:</font></strong> Qualitative results on the test set of SSCBench-KITTI-360. 
        </p>
      </div>
    </div>
    </div>

    <br/>

    <!-- <div class="container is-max-desktop">
      <div class="content has-text-justified">
      <h2 class="title is-3 has-text-centered" style="color:#3e91d0;"><p>Tracking Accuracy Evaluation</p></h2>
        
        <img style="width: 70%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/cg-slam/images/tracking-accuracy-new.jpg"
        alt="tracking-accuracy."/>
        <p>
        We report the localization accuracy of our proposed CG-SLAM across different datasets. In the synthetic Replica dataset, e surpass all other methods by a notable margin around
        25%~75%. In the real-world TUM and ScanNet datasets, despite being affected by noisy and sparse depth information, our method can still achieve better or competitive performance, which further demonstrates its generalization and superiority.
        </h2>
      </div>
      </div> -->

    <!-- <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <h2 class="title is-3 has-text-centered"><p>Dense Reconstruction </p></h2>
        <video id="functions" autoplay controls muted loop playsinline height="100%">
          <source src="./video/office1-full.mp4"
                  type="video/mp4">
        </video>
        <p>
          Our representation support a series of editing functionalities, including a mesh-guided geometry editing, designated texture editing with texture swapping of two objects, texture filling with materials from pre-captured objects, and texture painting by transferring user-paints from 2D image to 3D field.
        </p>
      </div>
      </div> -->
    <br/>

    <div class="container is-max-desktop">
    <div class="content has-text-justified">
    <h2 class="title is-3 has-text-centered" style="color:#ffa200;"><p>Scene Semantic Completion &rarr; Quantitative Performance</p></h2>
        <div class="has-text-centered">
          <img style="width: 100%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/table-1.jpg"
          alt="ssc."/>
        </div>
      <p>
        <strong><font size="4">  Annotations:</font></strong> Quantitative results on the validation set of SemanticKITTI. &#9733; denotes the scene layout structures.
      </p>

        <div class="has-text-centered">
          <img style="width: 100%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/sgformer/images/table-2.jpg"
          alt="ssc."/>
        </div>
      <p>
        <strong><font size="4">  Annotations:</font></strong> Quantitative results on the test set of SSCBench-KITTI-360. &#9733; denotes the scene layout structures.
      </p>
    </div>
    </div>

    <!-- <br/>

    <div class="container is-max-desktop">
    <div class="content has-text-justified">
    <h2 class="title is-3 has-text-centered" style="color:#3e91d0;"><p>Reconstruction Evaluation</p></h2>
      <div class="has-text-centered">
        <img style="width: 100%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/cg-slam/images/reconstruction.png"
        alt="tracking-accuracy."/>
      </div>
      <p>
        It can be observed that our method can reconstruct a detailed mesh map while ensuring efficiency. 
        CG-slam has demonstrated competitive performance in mapping compared to the state-of-the-art NeRF-based PointSLAM, far surpassing other NeRF-based methods.
        It is worth noting that the Gaussian-based method neither has a global MLP nor a fully covered
        feature grid, as in Co-SLAM. Consequently, such a system exhibits a slightly weaker hole-filling ability in unobserved areas.
      </h2>
    </div>
    </div>

    <br/> -->



    <!-- <div class="container is-max-desktop">
    <div class="content has-text-justified">
    <h2 class="title is-3 has-text-centered" style="color:#3e91d0;"><p>Efficiency Evaluation</p></h2>
      <div class="has-text-centered">
        <img style="width: 55%;" src="https://raw.githubusercontent.com/hjr37/open_access_assets/main/cg-slam/images/efficiency.jpg"
        alt="rendering."/>
      </div>
      <p>
        We reported the tracking and mapping efficiency in terms of per-iteration time consumption and the total number of optimization iterations.
        The GPU-accelerated rasterizer and carefully designed pipeline allow our system to expand to a lightweight version, which can work with half-resolution images and perform tracking twice as fast as the full version at the
        cost of a slight decrease in accuracy. In addition, our lightweight version effectively alleviates the memory consumption while maintaining accuracy, which is a common problem in other concurrent works.
      </p>
      </h2>
    </div> -->

    


</section>

<hr/>

<section class="section" id="BibTeX">
  <div class="container content is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>@article{hu2024cg,
      title={CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field},
      author={Hu, Jiarui and Chen, Xianhao and Feng, Boyin and Li, Guanglin and Yang, Liangjing and Bao, Hujun and Zhang, Guofeng and Cui, Zhaopeng},
      journal={arXiv preprint arXiv:2403.16095},
      year={2024}
    }
    </code> -->
    <pre><code>@article{guo2025sgformersatellitegroundfusion3d,
    title={SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion},
    author={Xiyue Guo and Jiarui Hu and Junjie Hu and Hujun Bao and Guofeng Zhang},
    journal={arXiv preprint arXiv:2503.16825},
    year={2025}
}
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      Thanks to <a href="https://www.flaticon.com/" target="_blank">Flaticon</a> for providing beautiful icons.
      The website template is borrowed from <a href="https://hypernerf.github.io/" target="_blank">HyperNeRF</a>.
    </div>
  </div>
</footer>

<script>
  MathJax = {
    tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
  };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-F5RT7HMEN2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-F5RT7HMEN2');
</script>

<script type="text/javascript" src="./static/slick/slick.min.js"></script>
</body>
</html>
