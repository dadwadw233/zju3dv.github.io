<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Deep Active Contour for Real-time 6-DoF Object Tracking</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Deep Active Contour for Real-time 6-DoF Object Tracking</h2>
            <h4 style="color:#6e6e6e;">ICCV 2023</h4>
            <!-- <h5 style="color:#6e6e6e;"> (Oral Presentation and Best Paper Candidate)</h5> -->
            <hr>
            <h6> <a href="https://wanglongzju.github.io/wanglong.github.io/" target="_blank">Long Wang</a><sup>1*</sup>, 
                 <a target="_blank">Shen Yan</a><sup>3*</sup>, 
                 <a target="_blank">Jianan Zhen</a><sup>1</sup>, 
                <a target="_blank">Yu Liu</a><sup>3</sup>,
                <a target="_blank">Maojun Zhang</a><sup>3</sup>,
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>1,2</sup>,
                <a href="https://xzhou.me" target="_blank">Xiaowei Zhou</a><sup>2</sup>
            </h6>
            <p> <sup>1</sup>SenseTime Research &nbsp;&nbsp; 
                <sup>2</sup>Zhejiang University &nbsp;&nbsp;
                <sup>3</sup>National University of Defense Technology &nbsp;&nbsp;
                <br>
                <sup>*</sup> denotes equal contribution
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Deep_Active_Contours_for_Real-time_6-DoF_Object_Tracking_ICCV_2023_paper.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code" href="https://github.com/WangLongZJU/DeepAC" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code(Coming) </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="videos/DeepAC_sup.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="dataset" href="https://zjueducn-my.sharepoint.com/:f:/g/personal/zihaowang_zju_edu_cn/ElfzHE0sTXxNndx6uDLWlbYB-2zWuLfjNr56WxF11_DwSg?e=HDd14G" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Dataset </a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <h6 style="color:#8899a5"> TL;DR: DeepAC solves the problem of real-time 6-DoF object tracking from an RGB video with a CAD model on mobile device. </h6>
<!--             <video poster="images/header-vid-poster.png" width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid"> -->
            <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
                  <source src="videos/DeepAC_demo.mp4" type="video/mp4">
            </video> -->
            <video id="demo" width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls="" id="inspect_vid">
                  <source src="videos/DeepAC_demo.mp4" type="video/mp4">
            </video>

            <div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
            <br>
            </div>
              <!-- <br><br> -->
          <p class="text-justify">
            This paper solves the problem of real-time 6-DoF object tracking from an RGB video. Prior optimization-based methods optimize the object pose by aligning the projected model to the image based on handcrafted features, which is prone to suboptimal solutions. Recent learning-based methods use a deep network to predict the pose, which has limited generalizability or computational efficiency. We propose a learning-based active contour model to make the best use of both worlds. Specifically, given the initial pose, we project the object model to the image plane to obtain the initial contour and use a lightweight network to predict how the contour should move to match the true object boundary, which gives the gradients to optimize the object pose. We also devise an efficient optimization algorithm to train our model end-to-end with pose supervision. Experimental results on semi-synthetic and real-world 6-DoF object tracking datasets demonstrate that our model outperforms state-of-the-art methods by a substantial margin in pose accuracy, while achieving real-time performance on a mobile device. 
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- overview video -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Overview video (5 min)</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/wuMPaUTJuO0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
            </div>
        </div>
      </div>
    </div>
  </section>
  <br> -->


  <!-- Pipeline overview -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Pipeline overview</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/pipeline.png" alt="NeuralRecon Architechture">
            <hr style="margin-top:0px">
            <p class="text-justify">
              $\textbf{1.}$ The method uses an FPN-Lite CNN to extract multi-level features for the current cropped frame $_c\mathit{I}_k$, and represents the local region of the contour by a correspondence line model.
              $\textbf{2.}$ A contour feature map $_{ct}\mathbf{F}_k^s$ is built by sampling a cycle of correspondence lines upon the image feature map, followed by a boundary prediction module that predicts boundary location probability $_{ct}\mathbf{B}_k^s$.
              $\textbf{3.}$ A differentiable optimization layer is used to estimate the pose $\mathbf{P}_k^s$ in a coarse-to-fine manner. The superscript `s' refers to various feature levels.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

    <!-- Comparison with Atlas -->
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
              <h3> Tracking Results on Public Dataset without Reset </h3>
              <hr style="margin-top:0px">
              <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls="">
                <source src="videos/new_demo1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </section>
    <br>  

  <!-- Comparison with depth-based methods -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with depth-based methods</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/compare-depth-based.png" alt="Comparison with depth-based methods">
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgement</h3>
          <hr style="margin-top:0px">
          <p>
            We would like to specially thank to Reviewer 3 for the positive and constructive comments.
          </p>
          <hr>
      </div>
    </div>
  </div> -->

  <!-- citing -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
  @article{sun2022onepose,
    title={{OnePose}: One-Shot Object Pose Estimation without {CAD} Models},
    author = {Sun, Jiaming and Wang, Zihao and Zhang, Siyu and He, Xingyi and Zhao, Hongcheng and Zhang, Guofeng and Zhou, Xiaowei},
    journal={CVPR},
    year={2022},
   }
</code></pre>
      </div>
    </div>
  </div> -->

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            We would like to specially thank Reviewer 3 for the insightful and constructive comments.
            We would like to thank Sida Peng , Siyu Zhang and Qi Fang for the proof-reading.
          </p>
      </div>
    </div>
  </div> -->

  <!-- rec -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works from our group</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout our work on Transformer-based feature matching (<a href="http://zju3dv.github.io/loftr">LoFTR</a>) and human reconstruction (<a href="http://zju3dv.github.io/neuralbody">NeuralBody</a> and <a href="http://zju3dv.github.io/Mirrored-Human">Mirrored-Human</a>) in CVPR 2021.
          </p>
      </div>
    </div>
  </div> -->

  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>

