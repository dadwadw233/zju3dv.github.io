<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in High Latency Situations. Published in the VR 2022."/>
    <title>Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in High Latency Situations </title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <body style="background-color: #ffffff">
  <h1> </h1>
  </body>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">
              Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in High Latency Situations
            </h2>
            <h5 style="color:#6e6e6e;">VR 2022</h5>
            <hr>
            <h6> <a target="_blank">Hujun Bao</a><sup>1</sup>,
                 <a target="_blank">Weijian Xie</a><sup>1,2</sup>,
                <a target="_blank">Quanhao Qian</a><sup>2</sup>,
                <a target="_blank">Danpeng Chen</a><sup>1,2,3</sup>,
                <a target="_blank">Shangjin Zhai</a><sup>2</sup>,
                <a target="_blank">Nan Wang</a><sup>2,3</sup>,
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>1*</sup></h6>
            <p> <sup>1</sup>State Key Lab of CAD & CG, Zhejiang University&nbsp;&nbsp;
                <sup>2</sup>SenseTime Research
                <sup>3</sup>Tetras.AI
                <br>
                <sup>*</sup> denotes corresponding author
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/ismar21b-sub2280-cam-i5.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
                <div class="column">
                    <p class="mb-5"><a class="btn btn-large btn-light" href="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Robust%20Tightly-Coupled%20Visual-Inertial%20Odometry%20with%20Pre-built%20Maps%20in%20High%20Latency%20Situations-supp.pdf" role="button"  target="_blank">
                        <i class="fa fa-file"></i> Supplementary</a> </p>
                </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px;">
            <hr style="margin-top:0px">
            <img class="img-fluid"
              src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/teaser_image.jpg"
              alt="Architechture">
            <hr>
            <p class="text-justify">
              In this paper, we present a novel monocular visual-inertial odometry system with pre-built
              maps deployed on the remote server, which can robustly run in real-time on a mobile device
              even in high latency situations. By tightly coupling VIO with geometric priors from pre-built 
              maps, our system can tolerate the high latency and low frequency of global localization service, 
              which is especially suitable for practical applications when the localization service is deployed 
              on the remote server. Firstly, sparse point clouds are obtained from the dense mesh by the ray 
              casting method according to the localization results. The dense mesh can be reconstructed from the 
              point clouds generated by Structure-from-Motion. We directly use the sparse point clouds 
              in feature tracking and state update to suppress drift. In the process of feature tracking, 
              the high local accuracy of VIO is fully utilized to effectively remove outliers and make our system robust. 
              The experiments on EurocMav datasets and simulation datasets show that compared with state-of-the-art methods,
              our method can achieve better results in terms of both precision and robustness. 
              The effectiveness of the proposed method is further demonstrated through a real-time AR demo on a 
              mobile phone with the aid of visual localization on the remote server.  
            </p>
        </div>
      </div>
    </div>
  </section>
  <br> 

  <!-- system overview -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>System overview</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/system.png" alt="RTC-VIO System Overview" width="80%">
            <br>
            <br>
            <p class="text-justify">
              We use both the real-time poses and global localization poses to extract point clouds from the pre-built map. We propose a map management module and efficient data association strategy to deal with the localization latency and low-frequency localization. Then, we use different constraints to integrate structural information into VIO's state update for different types of map points. In addition, we analyze how to determine the degeneration state and how to recover from the degeneration state. Compared with previous works, our method can better tolerate the time delay and error of localization. 
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Results -->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Experiments</h3>
            <hr style="margin-top:0px">
            <p class="text-justify">
              We compare our method with several state-of-the-art methods on both real and simulated datasets.
              For the convenience of explanation, in the experiments, we name our
              basic VIO as BVIO, and the VIO coupled with the map as RTC-VIO.
              To evaluate the accuracy of the compared localization algorithms, we
compute the absolute pose error (APE). For BVIO, <a href="https://docs.openvins.com/" target="_blank">OpenVINS</a>, 
<a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank">ORB-SLAM3</a>, and <a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono" target="_blank">VINS-Mono</a>, we use evo
to estimate the transformation matrix from the estimated trajectory to the ground truth trajectory.
Meanwhile, it is unnecessary to estimate the transformation matrix for
tightly-coupled methods, which the methods already coupled with the
pre-built map.
            </p>
        </div>
      </div>
    </div>
  </section>

  <!--Neural Inertial Network-->
  <section>
      <div class="container">
          <div class="row">
              <div class="col text-center">
                  <h5 class="text-left"><li><b>Accuracy</b></li></h5>
                  <br>
                  <!-- <img class="img-fluid" src="images/cdf.png" width="70%"> -->
                  <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Accuracy-Euroc.png" width="48%">
                  <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Accuracy-Synthetic.png" width="48%">
                  <br>
                  <br>
                  <p class="text-justify">
                    The experiments on Euroc datasets and simulation datasets show that our method can achieve higher accuracy than state-of-the-art methods.
                  </p>
              </div>
          </div>
      </div>
  </section>

  <!--RNIN-VIO-->
  <section>
      <div class="container">
          <div class="row">
              <div class="col text-center">
                  <h5 class="text-left"><li><b>Robustness</b></li></h5>
                  <br>
                  <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Robust-Euroc.png" width="48%">
                  <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Robust-Euroc.png" width="48%">
                  <br>
                  <br>
                  <p class="text-justify">
                    Compared with GMM and DSL, our method is more robust to the changes of scenarios and the noise of the pre-built maps.
                  </p>
              </div>
          </div>
      </div>
  </section>
  
  <section>
      <div class="container">
          <div class="row">
              <div class="col text-center">
                  <h5 class="text-left"><li><b>Latency and Frequency</b></li></h5>
                  <br>
                  <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/delay.png" width="48%">
                  <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/latency.png" width="48%">
                  <br>
                  <br>
                  <p class="text-justify">
                    Under the condition of high delay and low localization frequency, our method can still achieve better accuracy than the traditional VIO.
                  </p>
              </div>
          </div>
      </div>
  </section>

  <br>

  <!-- Demo -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>RTC-VIO Demo</h3>
            <hr style="margin-top:0px">
            <br>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source
            src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Experiments-Our-Method.mp4" 
            type="video/mp4">
        </video>  
          
        </div>
        <!-- <div class="col text-center">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/demo02.mp4" type="video/mp4">
            </video>
        </div> -->
        <br>
        <br>
        <p class="text-justify">
          To better reflect the advantages of our method in practical applications,
          we have implemented an AR demo that can run in real-time on a mobile
          phone with the aid of visual localization on the remote server.
          The AR demo runs on a HUAWEI MATE20 PRO. We reconstructed
          two open scenes, an indoor scene and an outdoor scene, and the data
          used for scene reconstruction were panoramic images collected with a
          camera five months before the actual test. Therefore, both the environ-
          ment and the device have changed significantly, which is a big challenge
          for the localization algorithm and our coupled method. In addition,
          because the localization algorithm is deployed on the server, there is
          a network latency between 300 and 500 milliseconds per localization
          request. The localization interval is set to 1000ms. Due to the lack
          of ground truth, we can only verify drift accumulation through loop
          closure. For comparison, we also developed a loosely-coupled demo
          based on ARCore, which is called as ARCore-LC. The implemented
          details can be found in the supplementary document.  
          </p>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source
            src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Experiment-Hololens2.mp4" 
            type="video/mp4">
        </video>
        </div>
        <div class="col text-center">
            <img class="img-fluid" src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Hololens_ARCore_RTC.png" alt="RNIN-VIO System Overview" width="100%">
            <!-- <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/demo02.mp4" type="video/mp4">
            </video> -->
        </div>
        <br>
        <br>
        <p class="text-justify">
          We make extensive comparison with Hololens 2. Since
          Hololens is a closed source commercial software, we can only analyze
          it through real-time experiments. As far as we know, unlike our method,
          which can utilize the pre-built map generated by the SfM algorithm,
          Hololens relies on the map generated by itself. So the accuracy of
          the map which Hololens can use depends on its tracking accuracy. To
          visualize the trajectory of the Hololens, we install an App called Graffiti
          3D from the App Store. In the supplementary video of the Hololens
          test, we use the Graffiti 3D to draw the trajectory line as an AR effect.
          A person using HoloLens walked around the test scenario three times
          and recorded the AR result of Hololens 2. The purpose of the first
          round is to build the map. The following two rounds are designed to
          test the tracking accuracy of Hololens with the pre-built map created
          in the first round. We can see an apparent cumulative error in the first
          round from the video. Even if the loop closure occurs, the cumulative
          error is not completely eliminated. Repeating the same route still has
          almost the same accumulation drift and AR jumping problem.
        </p>
      </div>
    </div>
  </section>

<!-- overview video -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Overview Video</h3>
        <hr style="margin-top:0px">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source
            src="http://www.cad.zju.edu.cn/home/gfzhang/papers/tightly_coupled_vio/present.mp4" 
            type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>
<br>


<!-- <section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Overview Video</h3>
        <hr style="margin-top:0px">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%"
            src="https://raw.githubusercontent.com/terrancefox/open_access_assets/main/rtc-vio/Experiments-Our-Method.mp4" frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<br> -->

  <!-- citing -->
  <br>
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
  @article{bao2022robust,
    title={Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in High Latency Situations},
    author={Bao, Hujun and Xie, Weijian and Qian, Quanhao and Chen, Danpeng and Zhai, Shangjin and Wang, Nan and Zhang, Guofeng},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    volume={28},
    number={5},
    pages={2212--2222},
    year={2022},
    publisher={IEEE}
  }
  </code></pre>
      </div>
    </div>
  </div>

  <!-- ack -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            The authors would like to thank Youji Feng, Liyang Zhou, Fei Jiao,
            Mingxuan Jiang, Chongshan Sheng, Yuequ Cai for their kind help in
            the development of the global localization service and the real-time
            AR demo. This work was partially supported by NSF of China (No.
            61932003).
          </p>
      </div>
    </div>
  </div>

  <!-- rec -->
  <!-- 
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works from our group</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout our work on Transformer-based feature matching (<a href="http://zju3dv.github.io/loftr">LoFTR</a>) and human reconstruction (<a href="http://zju3dv.github.io/neuralbody">NeuralBody</a> and <a href="http://zju3dv.github.io/Mirrored-Human">Mirrored-Human</a>) in CVPR 2021.
          </p>
      </div>
    </div>
  </div>
  -->


  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        giteeLink.innerText = "mirror hosted in mainland China";
        giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
