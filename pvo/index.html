<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="PVO: Panoptic Visual Odometry"/>
    <title>PVO: Panoptic Visual Odometry</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <body style="background-color: #ffffff">
  <h1> </h1>
  </body>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">
              PVO: Panoptic Visual Odometry
            </h2>
            <h5 style="color:#6e6e6e;">Arxiv 2022</h5>
            <hr>
            <h6>
              <a href="https://ywcmaike.github.io/" target="_blank">Weicai Ye*</a><sup>1</sup>,
              <a href="" target="_blank">Xinyue Lan*</a><sup>1</sup>,
              <a href="https://github.com/Eric3778" target="_blank">Shuo Chen</a><sup>1</sup>,
              <a href="" target="_blank">Yuhang Ming</a><sup>2</sup>,
              <a href="" target="_blank">Xinyuan Yu</a><sup>1,3</sup>,
              <a href="http://www.cad.zju.edu.cn/home/bao/" target="_blank">Hujun Bao</a><sup>1</sup>,
              <a href="https://zhpcui.github.io/" target="_blank">Zhaopeng Cui</a><sup>1</sup>,
              <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>1†</sup>
            </h6>
            <p> 
                <sup>1</sup>State Key Lab of CAD & CG, Zhejiang University&nbsp;&nbsp;
                <sup>2</sup>Visual Information Laboratory, University of Bristol
                <sup>3</sup>Wuhan University
                <br>
                 * denotes equal contribution
                 <br> 
                 † denotes corresponding author
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2207.01610" role="button"
                    target="_blank">
                    <i class="fa fa-file"></i> Arxiv (with Supplementary) </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light"
                    href="doc/pvo_arxiv.pdf"
                    role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/zju3dv/pvo"
                    role="button" target="_blank">
                    <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid"
              src="imgs/pvo_teaser.jpg"
              alt="teaser_image">
            <hr>
            <p class="text-justify">
              We present a novel panoptic visual odometry framework, termed PVO, to achieve a more comprehensive modeling
               of the scene's motion, geometry, and panoptic segmentation information. PVO models visual odometry (VO) and 
               video panoptic segmentation (VPS) in a unified view, enabling the two tasks to facilitate each other. Specifically, 
               we introduce a panoptic update module into the VO module, which operates on the image panoptic segmentation. 
               This Panoptic-Enhanced VO module can trim the interference of dynamic objects in the camera pose estimation by 
               adjusting the weights of optimized camera poses. On the other hand, the VO-Enhanced VPS module improves the segmentation 
               accuracy by fusing the panoptic segmentation result of the current frame on the fly to the adjacent frames, using geometric 
               information such as camera pose, depth, and optical flow obtained from the VO module. These two modules contribute to each 
               other through a recurrent iterative optimization. Extensive experiments demonstrate that PVO outperforms 
               state-of-the-art methods in both visual odometry and video panoptic segmentation tasks.
            </p>
        </div>
      </div>
    </div>
  </section>

  <!-- system overview -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>System overview</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="imgs/pvo_framework.jpg" alt="PVO System Overview" width="80%">
            <br>
            <br>
            <p class="text-justify">
              <strong>Panoptic Visual Odometry Architecture</strong>. Our method consists of three modules, namely, an image panoptic segmentation module for system initialization (blue), 
              a Panoptic-Enhanced VO module (orange), and a VO-Enhanced VPS module (red). The last two modules contribute to each other 
              in a recurrent iterative manner.            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Results -->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Experiments</h3>
            <hr style="margin-top:0px">
            <p class="text-justify">
              We compare our method with several state-of-the-art methods for both two tasks.
              For visual odometry, we conduct experiments on three datasets with dynamic scenes: 
              VKITTI2, KITTI, and TUM RGBD dynamic sequences, to evaluate the accuracy of the camera trajectory, primarily using Absolute Trajectory Error. 
              For video panoptic segmentation, we evaluate the VPQ metric used in FuseTrack on VKITTI2, Cityscapes and VIPER datasets. 
            </p>
        </div>
      </div>
    </div>
  </section>

  <!--Neural Inertial Network-->
  <section>
      <div class="container">
          <div class="row">
              <div class="col text-center">
                  <h5 class="text-left"><li><b>Visual Odometry</b></li></h5>
                  <br>
                  <!-- <img class="img-fluid" src="images/cdf.png" width="70%"> -->
                  <img class="img-fluid" src="imgs/vo_vkitti2.jpg" width="48%", height="48%">
                  <img class="img-fluid" src="imgs/vo_tum.jpg" width="48%", height="48%">
                  <img class="img-fluid" src="imgs/vo_vkitti2_pose.jpg" width="48%", height="48%">
                  <img class="img-fluid" src="imgs/vo_kitti.jpg" width="48%", height="48%">
                  
                  <br>
                  <br>
                  <p class="text-justify">
                    As shown in Tab. 1 and Fig. 6, our PVO outperforms DROID-SLAM by a large margin except for the vkitti02 sequence.      
                    Compared with DROID-SLAM, we achieve nearly half of the pose estimation error in DROID-SLAM, shown in Fig. 7, which demonstrates good generalization ability of PVO.        
                    Tab. 2 demonstrates that our methods perform better on all datasets, compared with DROID-SLAM. We achieve the best results on 5 dataset out of 9 datasets. Note that PointCorr is a state-of-the-art RGB-D SLAM using Point Correlation, while ours only used monocular RGB video.    
                   </p>
              </div>
          </div>
      </div>
  </section>

  <section>
      <div class="container">
          <div class="row">
              <div class="col text-center">
                  <h5 class="text-left"><li><b>Video Panoptic Segmentation</b></li></h5>
                  <br>
                  <img class="img-fluid" src="imgs/vps_cityscapes.jpg">
                  <img class="img-fluid" src="imgs/vps_viper.jpg">
                  <img class="img-fluid" src="imgs/vps_vkitti2.jpg" >
                  <br>
                  <br>
                  <p class="text-justify">
                    We observe that our method with PanopticFCN outperforms the state-of-the-art method, achieving +1.6% VPQ higher than VPSNet-Track on Cityscapes-Val dataset.  
                    Compared with VPSNet-FuseTrack, our method with PanopticFCN achieves much higher scores (51.5VPQ vs. 48.4 VPQ) on VIPER dataset.
                    As shown in Tab. 5, the VO-Enhanced VPS module is effective in improving segmentation accuracy and tracking consistency. 
                                    </p>
              </div>
          </div>
      </div>
  </section>
  

  <br>

  <!-- Demo -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>PVO Demo</h3>
            <hr style="margin-top:0px">
            <br>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source
            src="videos/pvo_demo.mp4" 
            type="video/mp4">
        </video>  
          
        </div>
        <!-- <div class="col text-center">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/demo02.mp4" type="video/mp4">
            </video>
        </div> -->
        <br>
        <br>
        <p class="text-justify">
          Panoptic Visual Odometry takes a monocular video as input and outputs the panoptic 3D map while simultaneously localizes the camera itself with respect to the map.
          We show the panoptic 3D map produced by our method. The red triangle indicates the camera pose, and different colors indicate different instances.
          </p>
      </div>
    </div>
  </section>
  

<!-- overview video -->
<!-- <section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Overview Video</h3>
        <hr style="margin-top:0px">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source
            src="videos/pvo_overview.mp4" 
            type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section> -->
<br>


  <!-- citing -->
  <br>
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
  @article{Ye2022PVO,
    title={PVO: Panoptic Visual Odometry},
    author={Ye, Weicai and Lan, Xinyue and Chen, Shuo and Ming, Yuhang and Yu, Xinyuan and Bao, Hujun and Cui, Zhaopeng and Zhang, Guofeng},
    booktitle={arxiv}, 
    year={2022}
  }
  </code></pre>
      </div>
    </div>
  </div>

  <!-- ack -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            This work was partially supported by NSF of China (No. 61932003) and ZJU-SenseTime Joint Lab of 3D Vision.
          </p>
      </div>
    </div>
  </div>

  <!-- rec -->
  <!-- 
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works from our group</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout our work on Transformer-based feature matching (<a href="http://zju3dv.github.io/loftr">LoFTR</a>) and human reconstruction (<a href="http://zju3dv.github.io/neuralbody">NeuralBody</a> and <a href="http://zju3dv.github.io/Mirrored-Human">Mirrored-Human</a>) in CVPR 2021.
          </p>
      </div>
    </div>
  </div>
  -->


  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        giteeLink.innerText = "mirror hosted in mainland China";
        giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
